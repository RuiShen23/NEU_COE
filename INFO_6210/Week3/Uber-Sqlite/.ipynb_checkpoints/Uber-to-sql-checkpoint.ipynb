{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uber Data Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the research would be to build an application to optimize Uber drivers’ rides in Boston. With this application Uber drivers should have a higher driving frequency, leading to \n",
    "higher profit margins in less time. Factors and assumptions to take into consideration include but are not limited to customer pickup frequency, time of day, weather, and popular events happening in the city. Through this project, we aim to learn and apply predictive models like time series analysis, as well as machine learning methods such as Support Vector Machines, K- Nearest Neighbors, and Naïve Bayes. Uber drivers only have a finite amount time for driving, so why not make the most out of their driving time? \n",
    "Here, we focus on the Uber ride in Boston and Cambridge areas.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background -\n",
    "Being an Uber driver may sound lucrative, but how real is it and can the time utilization be maximized as they also need to pay for expenses for running the car or its insurance. An Uber driver may happen to be driving at a particular location, but there is no guarantee that there are passengers there, if any. Logically speaking, it is better to have accurate predictions for an Uber driver to decide upon his/her own schedule, maximize earnings in a shorter time, and on top of that knowing all the best driving locations where ride-hailing customers are densest. Conducting data analysis, training predictive models, and building an application would come in handy for the drivers, all in which are the aim of the project. \n",
    "\n",
    "This notebook deals with obtaining responses from Uber API and storing them into SQLite database for further anallysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset sources-  \n",
    "Uber api -https://developer.uber.com/\n",
    "Uber Rides Python SDK (beta) -https://github.com/uber/rides-python-sdk\n",
    "Yelp data on uber rides in boston - https://www.yelp.com/search?find_desc=uber&find_loc=Boston%2C+MA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Step - To query yelp api to get location latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Storing yelp data in sqllite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "yelp_db_nw = 'yelp_db_nw.sqlite' \n",
    "conn = sqlite3.connect(yelp_db_nw) \n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in conn.execute(\"pragma table_info('yelp_reviews')\").fetchall(): #Checking table contents\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'yelp_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b01bec5d1fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_nwf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yelp_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reading yelp csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_nwf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'yelp_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data_nwf = pd.read_csv(\"yelp_data.csv\", encoding ='latin-1') #reading yelp csv\n",
    "data_nwf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nwf.to_sql('yelp_reviews',             # Name of the table.\n",
    "             con=conn,                    # The handle to the file that is set up.\n",
    "             if_exists='replace',         # Overwrite, append, or fail.\n",
    "             index=False)                 # Add index as column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the latitude and longitude obtained from yelp api to query uber api\n",
    "Also, the minimum distance defined for a profitable ride is 1 mile. This is a straight line distance calculated through Geopy API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now hit Uber API..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from uber_rides.session import Session\n",
    "from uber_rides.client import UberRidesClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "session = Session(server_token='Uvu3eEPnLtPKCbTU7KrCko5jo1ua4CVgYAqd0JfO')\n",
    "client = UberRidesClient(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_first(): #function to query to select random first location\n",
    "    df = pd.read_sql('SELECT yelp_id, coordinates__latitude, coordinates__longitude FROM yelp_reviews where yelp_id IN (SELECT yelp_id FROM yelp_reviews ORDER BY RANDOM() LIMIT 1)', con=conn)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_second(): #function to query to select randon second location\n",
    "    df_2 = pd.read_sql('SELECT yelp_id, coordinates__latitude, coordinates__longitude FROM yelp_reviews where yelp_id IN (SELECT yelp_id FROM yelp_reviews ORDER BY RANDOM() LIMIT 1)', con = conn)\n",
    "    df_2.head()\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_first() # first query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_second() #second query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime  #first to define date time for east zone which is the area of interest\n",
    "import json\n",
    "import os\n",
    "\n",
    "class EST5EDT(datetime.tzinfo):\n",
    "\n",
    "    def utcoffset(self, dt):\n",
    "        return datetime.timedelta(hours=-5) + self.dst(dt)\n",
    "\n",
    "    def dst(self, dt):\n",
    "        d = datetime.datetime(dt.year, 3, 8)        #2nd Sunday in March\n",
    "        self.dston = d + datetime.timedelta(days=6-d.weekday())\n",
    "        d = datetime.datetime(dt.year, 11, 1)       #1st Sunday in Nov\n",
    "        self.dstoff = d + datetime.timedelta(days=6-d.weekday())\n",
    "        if self.dston <= dt.replace(tzinfo=None) < self.dstoff:\n",
    "            return datetime.timedelta(hours=1)\n",
    "        else:\n",
    "            return datetime.timedelta(0)\n",
    "\n",
    "    def tzname(self, dt):\n",
    "        return 'EST5EDT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actual_second(): #function to hit Uber API and store the result in a JSON file for further analysis\n",
    "    df_1 = find_first()\n",
    "    df_2 = find_second()\n",
    "    from geopy.distance import vincenty\n",
    "    start_loc = (df_1['coordinates__latitude'][0], df_1['coordinates__longitude'][0])\n",
    "  \n",
    "    end_loc = (df_2['coordinates__latitude'][0], df_2['coordinates__longitude'][0])\n",
    "   \n",
    "    distance = vincenty(start_loc, end_loc).miles\n",
    "    if(distance > 1):\n",
    "        \n",
    "        response = client.get_price_estimates(\n",
    "        start_latitude= df_1['coordinates__latitude'][0],\n",
    "        start_longitude= df_1['coordinates__longitude'][0],\n",
    "        end_latitude=  df_2['coordinates__latitude'][0],\n",
    "        end_longitude= df_2['coordinates__longitude'][0],\n",
    "        seat_count=2\n",
    "        )\n",
    "        prices = response.json.get(\"prices\")\n",
    "       \n",
    "        dt = datetime.datetime.now(tz=EST5EDT()) \n",
    "        \n",
    "        for price in prices:\n",
    "            price[\"time\"]= dt.strftime('%H:%M:%S')\n",
    "            price[\"Date-time\"] = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            price[\"start_latitude\"] = df_1['coordinates__latitude'][0]\n",
    "            price[\"start_longitude\"] = df_1['coordinates__longitude'][0]\n",
    "            price[\"end_latitude\"] = df_2['coordinates__latitude'][0]\n",
    "            price[\"end_longitude\"] = df_2['coordinates__longitude'][0]\n",
    "        \n",
    "        df = pd.DataFrame(prices)\n",
    "        df = df.append(df)\n",
    "        out = df.to_json(orient='records')\n",
    "        with open('uber_data.json', 'w+') as outfile:\n",
    "            json.dump(out, outfile)\n",
    "            outfile.close()\n",
    "    else:\n",
    "        actual_second() \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_second() # Call to above function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cron job was set up by loading the uber API query script on the server to collect real time data. The result were appended to a json file and used for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = json.load(open('uber_13_12_17.json')) # Read the files retrived from server\n",
    "dj = pd.read_json(j)\n",
    "df_uber = pd.DataFrame(dj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_uber(j):\n",
    "    p=os.path.join(\"\", j)\n",
    "    print (p)\n",
    "    with open(p, 'rU') as f:\n",
    "      data = [json.loads(row) for row in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# Will create uber_db.sqlite if it doesn't exist.\n",
    "uber_db = 'uber_db.sqlite' \n",
    "conn = sqlite3.connect(uber_db) \n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_uber.to_sql('uber_data',              # Name of the table.\n",
    "             con=conn,                    # The handle to the file that is set up.\n",
    "             if_exists='replace',         # Overwrite, append, or fail.\n",
    "             index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row in conn.execute(\"pragma table_info('uber_data')\").fetchall(): # Check contents of the table\n",
    "    print (row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM uber_data', con=conn) #read uber table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.read_sql('SELECT * FROM yelp_reviews', con =conn) # Check yelp review table\n",
    "df_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use joins to query and analyses result of both Uber and Yelp table. A joion example to find price estimates, uber ride type and category of the place of visit is illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_j = pd.read_sql('SELECT uber_data.display_name, uber_data.product_id, uber_data.estimate, yelp_reviews.location__address1, yelp_reviews.categories__title FROM uber_data INNER JOIN yelp_reviews ON uber_data.end_latitude=yelp_reviews.coordinates__latitude', con =conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_j.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_j.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
